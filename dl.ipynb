{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    return 1/ (1+np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x*(1-x)\n",
        "\n",
        "def mean_square_error_loss(y_true,y_pred):\n",
        "  return np.mean((y_true - y_pred)**2) # Use subtraction for MSE\n",
        "\n",
        "input = np.array([[0,0],[0,1],[1,0],[1,1]]) # Correct the input array format\n",
        "output = np.array([[0],[1],[1],[0]]) # Correct the output array format\n",
        "\n",
        "input_size = 2\n",
        "hidden_size = 2\n",
        "output_size = 1\n",
        "\n",
        "weights_input_hidden = np.random.rand(input_size,hidden_size)\n",
        "bias_hidden = np.random.rand(hidden_size)\n",
        "weights_hidden_output = np.random.randn(hidden_size,output_size)\n",
        "bias_output = np.random.rand(output_size)\n",
        "\n",
        "learning_rate = 0.1\n",
        "epochs = 20000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  hidden_input = np.dot(input,weights_input_hidden)+bias_hidden\n",
        "  hidden_output = sigmoid(hidden_input)\n",
        "\n",
        "  final_input = np.dot(hidden_output,weights_hidden_output)+bias_output\n",
        "  final_output = sigmoid(final_input)\n",
        "\n",
        "  loss = mean_square_error_loss(output,final_output)\n",
        "\n",
        "  error_output = final_output - output # Fix variable name 'outputs'\n",
        "  gradient_output = error_output*sigmoid_derivative(final_output)\n",
        "\n",
        "  error_hidden = np.dot(gradient_output,weights_hidden_output.T)\n",
        "  gradient_hidden = error_hidden*sigmoid_derivative(hidden_output)\n",
        "\n",
        "  weights_hidden_output -= learning_rate*np.dot(hidden_output.T,gradient_output)\n",
        "  bias_output -= learning_rate*np.sum(gradient_output,axis=0)\n",
        "  weights_input_hidden -= learning_rate*np.dot(input.T,gradient_hidden)\n",
        "  bias_hidden -= learning_rate*np.sum(gradient_hidden,axis=0)\n",
        "\n",
        "  if (epoch + 1)%2000 == 0: # Correct indentation\n",
        "    print(f\"Epoch:{epoch + 1},loss:{loss:.6f}\")\n",
        "\n",
        "results = [] # Fix variable name 'result'\n",
        "for input_pair in input: # Fix variable name 'inputs', iterate over 'input'\n",
        "  hidden_input = np.dot(input_pair,weights_input_hidden)+bias_hidden\n",
        "  hidden_output = sigmoid(hidden_input)\n",
        "  final_input = np.dot(hidden_output,weights_hidden_output)+bias_output\n",
        "  final_output = sigmoid(final_input)\n",
        "  results.append((input_pair,np.round(final_output[0],2))) # Append a tuple\n",
        "print(results)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSkv3mvJH4o1",
        "outputId": "1a16e65b-bb56-427c-fd4f-195871339e64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:2000,loss:0.249978\n",
            "Epoch:4000,loss:0.246362\n",
            "Epoch:6000,loss:0.136835\n",
            "Epoch:8000,loss:0.013654\n",
            "Epoch:10000,loss:0.005297\n",
            "Epoch:12000,loss:0.003146\n",
            "Epoch:14000,loss:0.002205\n",
            "Epoch:16000,loss:0.001685\n",
            "Epoch:18000,loss:0.001358\n",
            "Epoch:20000,loss:0.001135\n",
            "[(array([0, 0]), 0.04), (array([0, 1]), 0.97), (array([1, 0]), 0.97), (array([1, 1]), 0.03)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJa0_WFj9aBd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sq1E_MurAAnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EWi4Zi8HAAx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNet(object):\n",
        "    RNG = np.random.default_rng()\n",
        "\n",
        "    def __init__(self, topology:list[int] = []):\n",
        "        self.topology = topology\n",
        "        self.weight_mats = []\n",
        "        self._init_matrices()\n",
        "\n",
        "    def _init_matrices(self):\n",
        "        #-- set up matrices\n",
        "        if len(self.topology) > 1:\n",
        "            j = 1\n",
        "            for i in range(len(self.topology)-1):\n",
        "                num_rows = self.topology[i]\n",
        "                num_cols = self.topology[j] # Dedent this line to align with num_rows\n",
        "                mat = self.RNG.random(size=(num_rows, num_cols)) # Dedent to match the for loop level\n",
        "                self.weight_mats.append(mat)\n",
        "                j += 1\n",
        "\n",
        "    # ... rest of the code ..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "UAub3iLHBb8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ncLWeRZRAA0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiclass Classification\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([0.8, 0.6, 0.7])\n",
        "y = np.array([0, 1, 0])\n",
        "w1 = np.array([[0.2, 0.4, 0.1],\n",
        "              [0.5, 0.3, 0.2],\n",
        "              [0.3, 0.7, 0.8]])\n",
        "w2 = np.array([[0.6, 0.4, 0.5],\n",
        "              [0.1, 0.2, 0.3],\n",
        "              [0.3, 0.7, 0.2]])\n",
        "b1 = np.array([0.1, 0.2, 0.3])\n",
        "b2 = np.array([0.1, 0.2, 0.3])\n",
        "print(f\"\\nOld weights (w1):\\n{w1}\")\n",
        "print(f\"\\nOld biases (b1):\\n{b1}\")\n",
        "print(f\"\\nOld weights (w2):\\n{w2}\")\n",
        "print(f\"\\nold biases (b2):\\n{b2}\")\n",
        "\n",
        "# alpha\n",
        "learning_rate = 0.01\n",
        "epochs = 10000\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(0,x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "  return 1*(x>0)\n",
        "\n",
        "def softmax(A):\n",
        "    exps = np.exp(A - np.max(A, axis=-1, keepdims=True))\n",
        "    return exps / np.sum(exps, axis=-1, keepdims=True)\n",
        "\n",
        "# Loss = summation(y_true * log(y_pred))\n",
        "def lossFunction(y_true,y_pred):\n",
        "  return -np.sum(y_true*np.log(y_pred))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  # Forward propagation\n",
        "\n",
        "  # h1 = w1(x)+b1\n",
        "  h1 = np.dot(x, w1)+b1\n",
        "  # a1 = sig(h1)\n",
        "  a1 = relu(h1)\n",
        "\n",
        "  # h2 = w2(a1)+b2\n",
        "  h2 = np.dot(a1, w2)+b2\n",
        "  # a2 = softmax(h2)\n",
        "  a2 = softmax(h2)\n",
        "\n",
        "  # L = (h2-y)^2\n",
        "  loss = lossFunction(y, a2)\n",
        "  #error = a2 - y\n",
        "  #Error = (a2 - y)\n",
        "  error = a2 - y\n",
        "\n",
        "print(error)\n",
        "print(a2)\n",
        "print(loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ts7vtNyDAA3p",
        "outputId": "40808769-8b2f-468b-cf9c-b3cefa7e27d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Old weights (w1):\n",
            "[[0.2 0.4 0.1]\n",
            " [0.5 0.3 0.2]\n",
            " [0.3 0.7 0.8]]\n",
            "\n",
            "Old biases (b1):\n",
            "[0.1 0.2 0.3]\n",
            "\n",
            "Old weights (w2):\n",
            "[[0.6 0.4 0.5]\n",
            " [0.1 0.2 0.3]\n",
            " [0.3 0.7 0.2]]\n",
            "\n",
            "old biases (b2):\n",
            "[0.1 0.2 0.3]\n",
            "[ 0.25502746 -0.58413061  0.32910315]\n",
            "[0.25502746 0.41586939 0.32910315]\n",
            "0.8773840448515435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # d(L)/d(w2) = Error * a1\n",
        "  d_L_d_w2 = error.dot(a1.T)\n",
        "\n",
        "  # d(L)/d(b2) = Error\n",
        "  d_L_d_b2 = np.mean(error, axis=0)\n",
        "\n",
        "  d_L_d_hidden = np.dot(error, w2.T)\n",
        "  d_hidden_d_input = relu_derivative(h1)\n",
        "\n",
        "  # d(L)/d(w1) = Error * w2 * x\n",
        "  d_L_d_w1 = np.dot(x.T, d_hidden_d_input * d_L_d_hidden)\n",
        "\n",
        "  # d(L)/d(b1) = Error * w2\n",
        "  d_L_d_b1 = np.mean(d_hidden_d_input * d_L_d_hidden, axis=0)\n",
        "\n",
        "\n",
        "  # Update weights\n",
        "  w2 -= learning_rate * d_L_d_w2\n",
        "  b2 -= learning_rate * d_L_d_b2\n",
        "\n",
        "  w1 -= learning_rate *d_L_d_w1\n",
        "  b1 -= learning_rate * d_L_d_b1\n",
        "\n",
        "  if (epoch + 1) % 1000 == 0:\n",
        "    print(f\"Epoch:{epoch + 1}, Loss: {loss:.4f}\")\n",
        "\n",
        "\n",
        "# Print final outputs\n",
        "print(\"\\nFinal outputs (a2):\\n\", a2)\n",
        "print(\"Final Loss:\\n\", loss)\n",
        "\n",
        "results = []\n",
        "# for input_pair in inputs:\n",
        "#           hidden_input = np.dot(input_pair, weights_input_hidden)+bias_hidden\n",
        "#           hidden_output = sigmoid(hidden_input)\n",
        "#           final_input = np.dot(hidden_output, weights_hidden_output)+bias_output\n",
        "#           final_output = sigmoid(final_input)\n",
        "#           results.append((input_pair,np.round(final_output[0],2)))\n",
        "# print(results)\n",
        "\n",
        "print(f\"\\nUpdated weights (w1):\\n{w1}\")\n",
        "print(f\"\\nUpdated biases (b1):\\n{b1}\")\n",
        "print(f\"\\nUpdated weights (w2):\\n{w2}\")\n",
        "print(f\"\\nUpdated biases (b2):\\n{b2}\")\n",
        "print(f\"\\nOutput (a2) after epoch {epoch + 1}:\\n{a2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyQCF_VwOpmO",
        "outputId": "c30e6b4e-ae56-4565-913d-9e928e6deaf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:10000, Loss: 0.8774\n",
            "\n",
            "Final outputs (a2):\n",
            " [0.25502746 0.41586939 0.32910315]\n",
            "Final Loss:\n",
            " 0.8773840448515435\n",
            "\n",
            "Updated weights (w1):\n",
            "[[0.20115017 0.40115017 0.10115017]\n",
            " [0.50115017 0.30115017 0.20115017]\n",
            " [0.30115017 0.70115017 0.80115017]]\n",
            "\n",
            "Updated biases (b1):\n",
            "[0.10058413 0.20058413 0.30058413]\n",
            "\n",
            "Updated weights (w2):\n",
            "[[0.60149895 0.40149895 0.50149895]\n",
            " [0.10149895 0.20149895 0.30149895]\n",
            " [0.30149895 0.70149895 0.20149895]]\n",
            "\n",
            "Updated biases (b2):\n",
            "[0.1 0.2 0.3]\n",
            "\n",
            "Output (a2) after epoch 10000:\n",
            "[0.25502746 0.41586939 0.32910315]\n"
          ]
        }
      ]
    }
  ]
}